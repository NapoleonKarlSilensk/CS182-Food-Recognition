# å®éªŒç®¡ç†ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## ğŸ“š ç³»ç»Ÿç®€ä»‹

å®éªŒç®¡ç†ç³»ç»Ÿä¼šè‡ªåŠ¨ä¸ºæ¯æ¬¡å®éªŒåˆ›å»ºç‹¬ç«‹æ–‡ä»¶å¤¹ï¼Œä¿å­˜æ‰€æœ‰è®­ç»ƒæ•°æ®ã€æ¨¡å‹ã€å¯è§†åŒ–å›¾è¡¨ç­‰ï¼Œæ–¹ä¾¿åç»­å†™è®ºæ–‡å’Œå¤ç°ç»“æœã€‚

---

## ğŸ“ å®éªŒæ–‡ä»¶å¤¹ç»“æ„

æ¯æ¬¡å®éªŒä¼šåˆ›å»ºå¦‚ä¸‹ç»“æ„ï¼š

```
experiments/
â”œâ”€â”€ INDEX.md                              # æ‰€æœ‰å®éªŒçš„ç´¢å¼•åˆ—è¡¨
â”‚
â”œâ”€â”€ EXP002_baseline_cnn_sample_20251220_032418/
â”‚   â”œâ”€â”€ README.md                         # å®éªŒè¯´æ˜
â”‚   â”œâ”€â”€ experiment_metadata.json          # å®Œæ•´å…ƒæ•°æ®
â”‚   â”œâ”€â”€ plots/                            # å¯è§†åŒ–å›¾è¡¨
â”‚   â”‚   â”œâ”€â”€ training_curves.png           # è®­ç»ƒ/éªŒè¯æ›²çº¿
â”‚   â”‚   â””â”€â”€ confusion_matrix.png          # æ··æ·†çŸ©é˜µ
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ best_model.h5                 # æœ€ä½³æ¨¡å‹
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ config.json                   # è¶…å‚æ•°é…ç½®
â”‚   â”‚   â”œâ”€â”€ model_summary.txt             # æ¨¡å‹æ¶æ„
â”‚   â”‚   â””â”€â”€ training_history.json         # è®­ç»ƒå†å²æ•°æ®
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ metrics.json                  # æ€§èƒ½æŒ‡æ ‡
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.npy          # æ··æ·†çŸ©é˜µåŸå§‹æ•°æ®
â”‚   â”‚   â”œâ”€â”€ classification_report.txt     # åˆ†ç±»æŠ¥å‘Š
â”‚   â”‚   â””â”€â”€ classification_report.json    # åˆ†ç±»æŠ¥å‘Š(JSON)
â”‚   â””â”€â”€ predictions/
â”‚       â”œâ”€â”€ test_a_predictions.csv        # test_aé¢„æµ‹ç»“æœ
â”‚       â””â”€â”€ test_b_predictions.csv        # test_bé¢„æµ‹ç»“æœ
â”‚
â””â”€â”€ EXP003_baseline_cnn_full_20251220_032418/
    â””â”€â”€ ... (åŒä¸Š)
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹å¼1: åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨ï¼ˆæ¨èï¼‰

```python
from experiment_logger import ExperimentLogger, create_experiments_index

# 1. åˆ›å»ºå®éªŒè®°å½•å™¨
logger = ExperimentLogger('yamnet_transfer_learning')

# 2. è®°å½•é…ç½®
config = {
    'model': 'YAMNet',
    'pretrained': True,
    'epochs': 50,
    'batch_size': 32,
    'learning_rate': 0.0001,
    'dataset': 'train_full'
}
logger.log_config(config)

# 3. è®°å½•æ¨¡å‹æ¶æ„
logger.log_model_summary(model)

# 4. è®­ç»ƒæ¨¡å‹
history = model.fit(...)

# 5. è®°å½•è®­ç»ƒå†å²ï¼ˆè‡ªåŠ¨ç»˜åˆ¶æ›²çº¿ï¼‰
logger.log_training_history(history)

# 6. è¯„ä¼°å¹¶è®°å½•æŒ‡æ ‡
test_loss, test_acc = model.evaluate(X_test, y_test)
metrics = {
    'validation_accuracy': test_acc,
    'validation_loss': test_loss,
    'training_time': '45 minutes'
}
logger.log_metrics(metrics)

# 7. è®°å½•æ··æ·†çŸ©é˜µ
from sklearn.metrics import confusion_matrix
y_pred = model.predict(X_test)
cm = confusion_matrix(y_true, y_pred)
logger.log_confusion_matrix(cm, class_names)

# 8. ä¿å­˜æ¨¡å‹
logger.save_model(model, 'best_model.h5')

# 9. ä¿å­˜é¢„æµ‹ç»“æœ
predictions_df = pd.DataFrame({'name': files, 'label': pred_labels})
logger.save_predictions(predictions_df, 'test_a_predictions.csv')

# 10. å®Œæˆå®éªŒ
logger.finalize(final_metrics=metrics)

# 11. æ›´æ–°å®éªŒç´¢å¼•
create_experiments_index()
```

### æ–¹å¼2: å½’æ¡£å·²æœ‰å®éªŒ

```python
from experiment_logger import ExperimentLogger
import shutil

logger = ExperimentLogger('my_past_experiment')

# è®°å½•é…ç½®å’ŒæŒ‡æ ‡
logger.log_config(my_config_dict)
logger.log_metrics(my_metrics_dict)

# å¤åˆ¶å·²æœ‰æ–‡ä»¶
shutil.copy('old_model.h5', 
            os.path.join(logger.folders['models'], 'best_model.h5'))
shutil.copy('training_plot.png',
            os.path.join(logger.folders['plots'], 'training_curves.png'))

logger.finalize()
```

---

## ğŸ“Š æŸ¥çœ‹å®éªŒç»“æœ

### æ–¹æ³•1: æµè§ˆå™¨æŸ¥çœ‹
æ‰“å¼€ `experiments/INDEX.md` æŸ¥çœ‹æ‰€æœ‰å®éªŒæ¦‚è§ˆ

### æ–¹æ³•2: æŸ¥çœ‹å•ä¸ªå®éªŒ
è¿›å…¥å®éªŒæ–‡ä»¶å¤¹ï¼Œæ‰“å¼€ `README.md`

### æ–¹æ³•3: ç¼–ç¨‹è¯»å–
```python
import json

# è¯»å–å®éªŒå…ƒæ•°æ®
with open('experiments/EXP003_baseline_cnn_full_xxx/experiment_metadata.json') as f:
    metadata = json.load(f)
    print(metadata['metrics'])

# è¯»å–è®­ç»ƒå†å²
with open('experiments/EXP003_baseline_cnn_full_xxx/logs/training_history.json') as f:
    history = json.load(f)
    print(history['val_accuracy'])
```

---

## ğŸ¯ å·²å½’æ¡£çš„å®éªŒ

### EXP002: Baseline CNN (train_sample)
- **å‡†ç¡®ç‡**: 36%
- **æ•°æ®é›†**: train_sample (1000æ ·æœ¬)
- **è¯´æ˜**: ç®€åŒ–3å±‚CNNï¼Œç”¨äºå¿«é€ŸéªŒè¯

### EXP003: Baseline CNN (full train) â­ å½“å‰åŸºçº¿
- **å‡†ç¡®ç‡**: 57.86%
- **æ•°æ®é›†**: train (7000æ ·æœ¬)
- **è¯´æ˜**: ä»å¤´è®­ç»ƒçš„3å±‚CNNï¼Œå½“å‰baseline

---

## ğŸ“ å†™è®ºæ–‡æ—¶çš„ä½¿ç”¨å»ºè®®

### 1. å®éªŒå¯¹æ¯”è¡¨æ ¼
ä» `INDEX.md` ç›´æ¥å¤åˆ¶è¡¨æ ¼åˆ°è®ºæ–‡

### 2. è®­ç»ƒæ›²çº¿å›¾
ä½¿ç”¨ `plots/training_curves.png`ï¼ˆ300 DPIï¼Œé€‚åˆè®ºæ–‡ï¼‰

### 3. æ··æ·†çŸ©é˜µ
ä½¿ç”¨ `plots/confusion_matrix.png`

### 4. æ€§èƒ½æŒ‡æ ‡
ä» `metrics/metrics.json` è¯»å–ç²¾ç¡®æ•°å€¼

### 5. è¶…å‚æ•°è¡¨æ ¼
ä» `logs/config.json` è·å–æ‰€æœ‰é…ç½®

### 6. å®éªŒå¤ç°
æ‰€æœ‰é…ç½®å’Œä»£ç éƒ½ä¿å­˜åœ¨å®éªŒæ–‡ä»¶å¤¹ï¼Œå¯å®Œå…¨å¤ç°

---

## ğŸ”§ è‡ªå®šä¹‰å’Œæ‰©å±•

### æ·»åŠ è‡ªå®šä¹‰å¯è§†åŒ–
```python
import matplotlib.pyplot as plt

# ç»˜åˆ¶ä½ çš„å›¾è¡¨
plt.figure()
# ... ç»˜å›¾ä»£ç  ...

# ä¿å­˜åˆ°å®éªŒæ–‡ä»¶å¤¹
plot_path = os.path.join(logger.folders['plots'], 'my_custom_plot.png')
plt.savefig(plot_path, dpi=300)
plt.close()
```

### è®°å½•é¢å¤–ä¿¡æ¯
```python
# å¯ä»¥åœ¨metadataä¸­æ·»åŠ ä»»ä½•ä¿¡æ¯
logger.metadata['my_custom_field'] = 'custom value'

# ä¿å­˜è‡ªå®šä¹‰æ–‡ä»¶
import pickle
with open(os.path.join(logger.folders['metrics'], 'custom_data.pkl'), 'wb') as f:
    pickle.dump(my_data, f)
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **æ¯æ¬¡é‡è¦å®éªŒéƒ½ä½¿ç”¨è®°å½•å™¨** - å…»æˆä¹ æƒ¯
2. **å®éªŒåç§°è¦æè¿°æ€§** - å¦‚ `yamnet_lr0001_augmented`
3. **åŠæ—¶å®Œæˆå®éªŒ** - è°ƒç”¨ `finalize()` ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
4. **å®šæœŸæ›´æ–°ç´¢å¼•** - è¿è¡Œ `create_experiments_index()`
5. **æ·»åŠ å®éªŒå¤‡æ³¨** - åœ¨metadataä¸­è®°å½•å®éªŒæƒ³æ³•å’Œè§‚å¯Ÿ

---

## ğŸ“ å†™è®ºæ–‡checklist

ä»å®éªŒæ–‡ä»¶å¤¹è·å–ï¼š
- [ ] è®­ç»ƒ/éªŒè¯æ›²çº¿å›¾
- [ ] æ··æ·†çŸ©é˜µ
- [ ] æ€§èƒ½æŒ‡æ ‡è¡¨æ ¼
- [ ] è¶…å‚æ•°é…ç½®è¡¨æ ¼
- [ ] æ¨¡å‹æ¶æ„è¯´æ˜
- [ ] å®éªŒå¯¹æ¯”è¡¨
- [ ] æ—¶é—´æˆæœ¬åˆ†æ

æ‰€æœ‰è¿™äº›éƒ½å·²è‡ªåŠ¨ä¿å­˜åœ¨å®éªŒæ–‡ä»¶å¤¹ä¸­ï¼
